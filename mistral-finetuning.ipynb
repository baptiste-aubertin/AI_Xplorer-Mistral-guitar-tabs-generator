{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install vllm\n",
    "# !pip install huggingface_hub\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install bitsandbytes\n",
    "# !pip install trl\n",
    "# !pip install autoawq\n",
    "# !pip install peft\n",
    "# !pip install wandb\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bapt/.pyenv/versions/3.9.9/envs/mistral_0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-26 05:03:59,177\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import vllm\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer\n",
    "from awq import AutoAWQForCausalLM\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"ai-xploiter\"  # name your W&B project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/bapt/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "login(\"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\")\n",
    "base_model = 'mistralai/Mistral-7B-Instruct-v0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset_without_empty_measure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data:pd.DataFrame, nb_mesures:int=2):\n",
    "    tablatures = pd.DataFrame(columns = [\"Key\",\"Style\",\"Tablatures1\",\"Tablatures2\"])\n",
    "    for i in range(len(data)):\n",
    "        tab = data.iloc[i][\"Tablatures\"]\n",
    "        tab = tab.split(\"\\n\")\n",
    "        tab = [x.split(\"|\") for x in tab]\n",
    "        \n",
    "        new_tab1 = []\n",
    "        new_tab2 = []\n",
    "\n",
    "        for k in range((len(tab[0])-2)//nb_mesures):\n",
    "            new_tablatures_1 = \"\"\n",
    "            new_tablatures_2 = \"\"\n",
    "            for j in range(len(tab)):\n",
    "                line = tab[j][0]\n",
    "                new_small_tab = []\n",
    "                new_small_tab.append(tab[j][1+k*nb_mesures:1+(k+1)*nb_mesures])\n",
    "                new_tab_text = ([\"|\".join(x) for x in new_small_tab])\n",
    "                new_tablatures_1 += line+'|'+new_tab_text[0]+'|\\n'\n",
    "                new_tablatures_2 += line+'|'+new_tab_text[0]+'|\\n'\n",
    "            new_tab1.append(\"[startt]\"+new_tablatures_1+\"[endt]\")\n",
    "            new_tab2.append(\"[startt]\"+new_tablatures_2+\"[endt]\")\n",
    "\n",
    "        for p in range(len(new_tab1) - 1):\n",
    "            if i < len(data):  # Ensure index i is within the bounds of data\n",
    "                row = {\n",
    "                    \"Key\": data.iloc[i][\"Key\"],\n",
    "                    \"Style\": data.iloc[i][\"Style\"],\n",
    "                    \"Original_Tablatures1\": new_tab2[p],\n",
    "                    \"Tablatures1\": new_tab1[p],\n",
    "                    \"Tablatures2\": new_tab2[p + 1]\n",
    "                }\n",
    "                tablatures = pd.concat([tablatures, pd.DataFrame([row])], ignore_index=True)\n",
    "            else:\n",
    "                print(f\"Index {p} is out of bounds for the data DataFrame.\")\n",
    "        \n",
    "    return tablatures\n",
    "df = preprocessing(df, nb_mesures=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##\n",
      "<s>[INST] Give me the guitar four measures following this one in the key G and in the style Bossa Nova. Stop generating after four measures. Here is my guitar measures :\n",
      "            \n",
      "[startt]e|--------------|--------------|--------------|--------------|\n",
      "B|--7-----------|--------------|--------------|--------------|\n",
      "G|--------------|--7-----9-----|-----5--------|-----4--------|\n",
      "D|--------------|--------------|--------------|--------------|\n",
      "A|--------------|--------------|--------------|--------------|\n",
      "E|--------------|--------------|--------------|--------------|\n",
      "[endt] [/INST]\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Get tokenizer and configure padding\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Add special token for continuation\n",
    "# new_special_tokens = [\"<CONTINUE>\"]\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': new_special_tokens})\n",
    "\n",
    "\n",
    "\n",
    "def generate_prompt(row, tokenizer:PreTrainedTokenizer, training: bool = False) -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Give me the guitar four measures following this one in the key {row['Key']} and in the style {row['Style']}. Stop generating after four measures. Here is my guitar measures :\n",
    "            \n",
    "{row.get('Tablatures1', '')}\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if training:\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": row[\"Tablatures2\"]}\n",
    "        ]\n",
    "    chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + content.strip() }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\"\"\"\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=not training)#, chat_template=chat_template)\n",
    "\n",
    "# Print an example\n",
    "print(\"##\")\n",
    "print(generate_prompt(df.sample(n=1).to_dict(orient=\"records\")[0], tokenizer=tokenizer, training=False))\n",
    "print(\"##\")\n",
    "# Prepare prompts for SFT\n",
    "df[\"prompt\"] = df.apply(generate_prompt, tokenizer=tokenizer, training=True, axis=1)\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Then, split the remaining data into validation and test sets\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create the DatasetDict\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32768, 4096)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bapt/.pyenv/versions/3.9.9/envs/mistral_0/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/bapt/.pyenv/versions/3.9.9/envs/mistral_0/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1990/1990 [00:00<00:00, 10433.53 examples/s]\n",
      "Map: 100%|██████████| 995/995 [00:00<00:00, 10677.61 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/bapt/.pyenv/versions/3.9.9/envs/mistral_0/lib/python3.9/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myulin-shi\u001b[0m (\u001b[33mhackathon-ai-xploiter\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bapt/mistral-competition/wandb/run-20240526_050413-b06fks5y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hackathon-ai-xploiter/ai-xploiter/runs/b06fks5y' target=\"_blank\">r64-length1024-batch128-mistralai/Mistral-7B-Instruct-v0.3</a></strong> to <a href='https://wandb.ai/hackathon-ai-xploiter/ai-xploiter' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hackathon-ai-xploiter/ai-xploiter' target=\"_blank\">https://wandb.ai/hackathon-ai-xploiter/ai-xploiter</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hackathon-ai-xploiter/ai-xploiter/runs/b06fks5y' target=\"_blank\">https://wandb.ai/hackathon-ai-xploiter/ai-xploiter/runs/b06fks5y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 90/200 10:06 < 12:37, 0.15 it/s, Epoch 2.78/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Input Tokens Seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.622500</td>\n",
       "      <td>1.528547</td>\n",
       "      <td>228160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.864700</td>\n",
       "      <td>0.811556</td>\n",
       "      <td>463296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.571400</td>\n",
       "      <td>0.582265</td>\n",
       "      <td>699392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>0.557080</td>\n",
       "      <td>916470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>0.537168</td>\n",
       "      <td>1146230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.518503</td>\n",
       "      <td>1379766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.499381</td>\n",
       "      <td>1586088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.479498</td>\n",
       "      <td>1817512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=find_all_linear_names(model),\n",
    ")\n",
    "\n",
    "# Define generic training configuration\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=4e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    warmup_steps=20,\n",
    "    max_steps=200,\n",
    "    output_dir=f\"./wandb/test-{int(time())}\",\n",
    "    run_name = f\"r64-length1024-batch128-{base_model}\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_accumulation_steps=2,\n",
    "\n",
    "    report_to=\"wandb\",\n",
    "    include_tokens_per_second=True,\n",
    "    include_num_input_tokens_seen=True,\n",
    "\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    dataset_text_field=\"prompt\",\n",
    "\n",
    "    max_seq_length=1024,\n",
    "\n",
    "    # packing=True,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "\n",
    "adapter_path = \"./app/data/adapter\"\n",
    "\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=False):\n",
    "    trainer.train()\n",
    "trainer.save_model(adapter_path)\n",
    "\n",
    "del model, peft_config, training_args, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]\n",
      "Unloading and merging model: 100%|██████████| 678/678 [00:14<00:00, 46.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./app/data/merge/tokenizer_config.json',\n",
       " './app/data/merge/special_tokens_map.json',\n",
       " './app/data/merge/tokenizer.model',\n",
       " './app/data/merge/added_tokens.json',\n",
       " './app/data/merge/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantize and save model\n",
    "merge_path = \"./app/data/merge\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model = model.merge_and_unload(progressbar=True, safe_merge=True)\n",
    "model.save_pretrained(merge_path)\n",
    "tokenizer.save_pretrained(merge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# gc.collect()\n",
    "\n",
    "# model = AutoAWQForCausalLM.from_pretrained(merge_path)\n",
    "# quant_config = {\n",
    "#     \"zero_point\": True,\n",
    "#     \"q_group_size\": 128,\n",
    "#     \"w_bit\": 4, \n",
    "#     \"version\": \"GEMM\",\n",
    "# }\n",
    "# model.quantize(tokenizer, quant_config=quant_config)\n",
    "# model.save_quantized(awq_path)\n",
    "# tokenizer.save_pretrained(awq_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:48:12 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='./app/data/model1', speculative_config=None, tokenizer='./app/data/model1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1028, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=./app/data/model1)\n",
      "INFO 05-26 04:48:16 model_runner.py:175] Loading model weights took 13.5005 GB\n",
      "INFO 05-26 04:48:16 gpu_executor.py:114] # GPU blocks: 5038, # CPU blocks: 2048\n",
      "INFO 05-26 04:48:18 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-26 04:48:18 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-26 04:48:25 model_runner.py:1017] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "# del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Inference \n",
    "merge_path = \"./app/data/model1\"\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model=merge_path,\n",
    "    max_model_len=1028,\n",
    "    # tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=.3,\n",
    "    disable_log_stats=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/996 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:08 metrics.py:334] Avg prompt throughput: 45.1 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 982 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%\n",
      "INFO 05-26 04:49:13 metrics.py:334] Avg prompt throughput: 6537.6 tokens/s, Avg generation throughput: 5197.3 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 740 reqs, GPU KV cache usage: 77.6%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1%|          | 7/996 [00:09<09:34,  1.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:19 metrics.py:334] Avg prompt throughput: 48.6 tokens/s, Avg generation throughput: 5263.8 tokens/s, Running: 228 reqs, Swapped: 0 reqs, Pending: 761 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  23%|██▎       | 232/996 [00:14<00:13, 55.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:24 metrics.py:334] Avg prompt throughput: 9137.0 tokens/s, Avg generation throughput: 4214.3 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 505 reqs, GPU KV cache usage: 64.1%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  26%|██▌       | 255/996 [00:17<00:42, 17.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:29 metrics.py:334] Avg prompt throughput: 565.3 tokens/s, Avg generation throughput: 5580.1 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 484 reqs, GPU KV cache usage: 92.1%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  28%|██▊       | 276/996 [00:24<01:41,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:34 metrics.py:334] Avg prompt throughput: 2288.4 tokens/s, Avg generation throughput: 4767.6 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 489 reqs, GPU KV cache usage: 11.7%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  49%|████▉     | 491/996 [00:29<00:18, 26.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:39 metrics.py:334] Avg prompt throughput: 6697.9 tokens/s, Avg generation throughput: 4745.5 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 248 reqs, GPU KV cache usage: 77.7%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  52%|█████▏    | 518/996 [00:35<01:41,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:44 metrics.py:334] Avg prompt throughput: 607.0 tokens/s, Avg generation throughput: 4977.6 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 234 reqs, GPU KV cache usage: 99.9%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  74%|███████▎  | 734/996 [00:39<00:02, 114.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:49 metrics.py:334] Avg prompt throughput: 8130.3 tokens/s, Avg generation throughput: 4407.8 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 59.6%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  75%|███████▌  | 750/996 [00:44<00:13, 18.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:54 metrics.py:334] Avg prompt throughput: 27.1 tokens/s, Avg generation throughput: 5637.9 tokens/s, Running: 244 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 88.8%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  80%|███████▉  | 792/996 [00:50<00:22,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 04:49:59 metrics.py:334] Avg prompt throughput: 865.7 tokens/s, Avg generation throughput: 4909.3 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 996/996 [00:51<00:00, 19.44it/s] \n"
     ]
    }
   ],
   "source": [
    "prompts = test_df.apply(generate_prompt, tokenizer=tokenizer, training=False, axis=1)\n",
    "\n",
    "sampling_params = vllm.SamplingParams(\n",
    "    n=1,\n",
    "    temperature=0.,\n",
    "    top_p=0.95,\n",
    "    max_tokens=256,\n",
    ")\n",
    "outputs = llm.generate(list(prompts), sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    test_df[f\"output_{i}\"] = [output.outputs[i].text.strip() if i < len(output.outputs) else None for output in outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Style</th>\n",
       "      <th>Tablatures1</th>\n",
       "      <th>Tablatures2</th>\n",
       "      <th>Original_Tablatures1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>output_0</th>\n",
       "      <th>output_1</th>\n",
       "      <th>output_2</th>\n",
       "      <th>output_3</th>\n",
       "      <th>output_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>C#</td>\n",
       "      <td>Singer-Songwriter</td>\n",
       "      <td>e|--5--4--------|-----6--------|-----2--------...</td>\n",
       "      <td>e|--2-----------|--5--6--------|--------------...</td>\n",
       "      <td>e|--5--4--------|-----6--------|-----2--------...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|-----1--------|-----2--------|-----2--------...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3477</th>\n",
       "      <td>F#</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>A</td>\n",
       "      <td>Rock</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>C#</td>\n",
       "      <td>Funk</td>\n",
       "      <td>e|-----6--6-----|-----6--------|-----6--8-----...</td>\n",
       "      <td>e|-----8---------|----------------|--8--------...</td>\n",
       "      <td>e|-----6--6-----|-----6--------|-----6--8-----...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|-----8--8-----|-----8--------|-----8--8-----...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>Bb</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>F#</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Bb</td>\n",
       "      <td>Singer-Songwriter</td>\n",
       "      <td>e|--------------|----------------|------------...</td>\n",
       "      <td>e|---------------|----------------|-----------...</td>\n",
       "      <td>e|--------------|----------------|------------...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|--------------|----------------|------------...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>Ab</td>\n",
       "      <td>Funk</td>\n",
       "      <td>e|--4-----------|--------------|--------------...</td>\n",
       "      <td>e|--------------|--4-----------|--4-----4-----...</td>\n",
       "      <td>e|--4-----------|--------------|--------------...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>E</td>\n",
       "      <td>Singer-Songwriter</td>\n",
       "      <td>e|-----0--------|--0--0--------|-----0--0-----...</td>\n",
       "      <td>e|-----0--0-----|-----0--0-----|--------0-----...</td>\n",
       "      <td>e|-----0--------|--0--0--------|-----0--0-----...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|-----0--0-----|--0--0--0-----|--0--0--0-----...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>B</td>\n",
       "      <td>Bossa Nova</td>\n",
       "      <td>e|--------------|-----5--------|--------------...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>e|--------------|-----5--------|--------------...</td>\n",
       "      <td>&lt;s&gt;[INST] Give me the guitar four measures fol...</td>\n",
       "      <td>e|--------------|--------------|--------------...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Key              Style  \\\n",
       "3641  C#  Singer-Songwriter   \n",
       "3477  F#               Jazz   \n",
       "3758   A               Rock   \n",
       "905   C#               Funk   \n",
       "2288  Bb               Jazz   \n",
       "...   ..                ...   \n",
       "2972  F#               Jazz   \n",
       "2614  Bb  Singer-Songwriter   \n",
       "1042  Ab               Funk   \n",
       "2202   E  Singer-Songwriter   \n",
       "1786   B         Bossa Nova   \n",
       "\n",
       "                                            Tablatures1  \\\n",
       "3641  e|--5--4--------|-----6--------|-----2--------...   \n",
       "3477  e|--------------|--------------|--------------...   \n",
       "3758  e|--------------|--------------|--------------...   \n",
       "905   e|-----6--6-----|-----6--------|-----6--8-----...   \n",
       "2288  e|--------------|--------------|--------------...   \n",
       "...                                                 ...   \n",
       "2972  e|--------------|--------------|--------------...   \n",
       "2614  e|--------------|----------------|------------...   \n",
       "1042  e|--4-----------|--------------|--------------...   \n",
       "2202  e|-----0--------|--0--0--------|-----0--0-----...   \n",
       "1786  e|--------------|-----5--------|--------------...   \n",
       "\n",
       "                                            Tablatures2  \\\n",
       "3641  e|--2-----------|--5--6--------|--------------...   \n",
       "3477  e|--------------|--------------|--------------...   \n",
       "3758  e|--------------|--------------|--------------...   \n",
       "905   e|-----8---------|----------------|--8--------...   \n",
       "2288  e|--------------|--------------|--------------...   \n",
       "...                                                 ...   \n",
       "2972  e|--------------|--------------|--------------...   \n",
       "2614  e|---------------|----------------|-----------...   \n",
       "1042  e|--------------|--4-----------|--4-----4-----...   \n",
       "2202  e|-----0--0-----|-----0--0-----|--------0-----...   \n",
       "1786  e|--------------|--------------|--------------...   \n",
       "\n",
       "                                   Original_Tablatures1  \\\n",
       "3641  e|--5--4--------|-----6--------|-----2--------...   \n",
       "3477  e|--------------|--------------|--------------...   \n",
       "3758  e|--------------|--------------|--------------...   \n",
       "905   e|-----6--6-----|-----6--------|-----6--8-----...   \n",
       "2288  e|--------------|--------------|--------------...   \n",
       "...                                                 ...   \n",
       "2972  e|--------------|--------------|--------------...   \n",
       "2614  e|--------------|----------------|------------...   \n",
       "1042  e|--4-----------|--------------|--------------...   \n",
       "2202  e|-----0--------|--0--0--------|-----0--0-----...   \n",
       "1786  e|--------------|-----5--------|--------------...   \n",
       "\n",
       "                                                 prompt  \\\n",
       "3641  <s>[INST] Give me the guitar four measures fol...   \n",
       "3477  <s>[INST] Give me the guitar four measures fol...   \n",
       "3758  <s>[INST] Give me the guitar four measures fol...   \n",
       "905   <s>[INST] Give me the guitar four measures fol...   \n",
       "2288  <s>[INST] Give me the guitar four measures fol...   \n",
       "...                                                 ...   \n",
       "2972  <s>[INST] Give me the guitar four measures fol...   \n",
       "2614  <s>[INST] Give me the guitar four measures fol...   \n",
       "1042  <s>[INST] Give me the guitar four measures fol...   \n",
       "2202  <s>[INST] Give me the guitar four measures fol...   \n",
       "1786  <s>[INST] Give me the guitar four measures fol...   \n",
       "\n",
       "                                               output_0 output_1 output_2  \\\n",
       "3641  e|-----1--------|-----2--------|-----2--------...     None     None   \n",
       "3477  e|--------------|--------------|--------------...     None     None   \n",
       "3758  e|--------------|--------------|--------------...     None     None   \n",
       "905   e|-----8--8-----|-----8--------|-----8--8-----...     None     None   \n",
       "2288  e|--------------|--------------|--------------...     None     None   \n",
       "...                                                 ...      ...      ...   \n",
       "2972  e|--------------|--------------|--------------...     None     None   \n",
       "2614  e|--------------|----------------|------------...     None     None   \n",
       "1042  e|--------------|--------------|--------------...     None     None   \n",
       "2202  e|-----0--0-----|--0--0--0-----|--0--0--0-----...     None     None   \n",
       "1786  e|--------------|--------------|--------------...     None     None   \n",
       "\n",
       "     output_3 output_4  \n",
       "3641     None     None  \n",
       "3477     None     None  \n",
       "3758     None     None  \n",
       "905      None     None  \n",
       "2288     None     None  \n",
       "...       ...      ...  \n",
       "2972     None     None  \n",
       "2614     None     None  \n",
       "1042     None     None  \n",
       "2202     None     None  \n",
       "1786     None     None  \n",
       "\n",
       "[996 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e|--------------|----------------|---------------|-----------------|\n",
      "B|--6--6--------|-----9---9------|-----9---------|--9---9---9------|\n",
      "G|--6--6--------|-----10--10-----|-----10--------|--10--10--10-----|\n",
      "D|--8--8--------|-----10--10-----|---------------|--10--10--10-----|\n",
      "A|--------------|----------------|---------------|-----------------|\n",
      "E|--------------|----------------|---------------|-----------------|\n",
      "\n",
      "=====\n",
      "e|----------------|----------------|----------------|----------------|\n",
      "B|--9--9--9--9-----|--9--9--9--9-----|--9--9--9--9-----|--9--9--9--9-----|\n",
      "G|--10--10--10-----|--10--10--10-----|--10--10--10-----|--10--10--10-----|\n",
      "D|--10--10--10-----|--10--10--10-----|--10--10--10-----|--10--10--10-----|\n",
      "A|-----------------|-----------------|-----------------|-----------------|\n",
      "E|-----------------|-----------------|-----------------|-----------------|\n",
      " ┆e|----------------|----------------|----------------|----------------|\n",
      "B|--9--9--9--9-----|--9--9--9--9-----|--9--9--9--9-----|--9--9--9--9-----|\n",
      "G|--10--10--10-----|--10--10--10\n"
     ]
    }
   ],
   "source": [
    "i=12\n",
    "\n",
    "print(test_df[\"Original_Tablatures1\"].iloc[i])\n",
    "print(\"=====\")\n",
    "print(test_df[\"output_0\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m vllm.entrypoints.openai.api_server --model ./app/data/merge --dtype auto --max-model-len 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e|--5--4--------|-----6--------|-----2--------|-----1--------|-----1--------|-----2--------|-----2--------|-----2--------|',\n",
       " 'B|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|',\n",
       " 'G|--------------|-----6--------|-----2--------|-----2--------|-----2--------|-----2--------|-----2--------|-----2--------|',\n",
       " 'D|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|',\n",
       " 'A|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|',\n",
       " 'E|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|',\n",
       " '']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def postprocess_output(input:str, output: str) -> str:\n",
    "    # find the length of measures\n",
    "    inputs = input.split(\"\\n\")\n",
    "    inputs = [x.split(\"|\") for x in inputs]\n",
    "    length_measures = len(inputs[0][1])\n",
    "\n",
    "    output = output.split(\"\\n\")\n",
    "    output_corrected = input.split(\"\\n\")\n",
    "\n",
    "    for i in range(len(output)//6):\n",
    "        # check \n",
    "        for j in range(6):\n",
    "            if output[i*6+j][0] != inputs[j][0]:\n",
    "\n",
    "                return output_corrected\n",
    "            measures = output[i*6+j].split(\"|\")\n",
    "            for k in range(1,len(measures)-1):\n",
    "                if len(measures[k]) != length_measures:\n",
    "                    return output_corrected\n",
    "            if len(measures[-1]) != 0:\n",
    "                return output_corrected\n",
    "        # add the measures\n",
    "        for j in range(6):\n",
    "            output_corrected[j] += output[i*6+j][2:]\n",
    "    return output_corrected\n",
    "\n",
    "\n",
    "ind = 0\n",
    "postprocess_output(input = test_df[\"Original_Tablatures1\"].iloc[ind], output=test_df[\"output_0\"].iloc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
